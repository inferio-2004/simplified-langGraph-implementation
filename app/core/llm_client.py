import os
import asyncio
import logging
from typing import Optional, Dict, Any
from groq import Groq
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

logger = logging.getLogger(__name__)


class GroqClient:
    """Async Groq client for LLM operations"""
    
    def __init__(self):
        self.api_key = os.getenv("GROQ_API_KEY")
        if not self.api_key:
            logger.warning("GROQ_API_KEY not found in environment")
            self.client = None
        else:
            self.client = Groq(api_key=self.api_key)
            logger.info("Groq client initialized successfully")
    
    def is_available(self) -> bool:
        """Check if Groq client is available"""
        return self.client is not None
    
    async def summarize_text(
        self, 
        text: str, 
        max_length: int = 200,
        model: str = "llama-3.1-8b-instant"  # Current supported model
    ) -> str:
        """Generate summary using Groq LLM"""
        if not self.is_available():
            raise ValueError("Groq client not available - check API key")
        
        try:
            prompt = f"""Summarize the following text in approximately {max_length} characters. Be concise but comprehensive:

{text}

Summary:"""

            # Create completion
            completion = self.client.chat.completions.create(
                model=model,
                messages=[{
                    "role": "user",
                    "content": prompt
                }],
                temperature=0.3,  # Lower temperature for more focused summaries
                max_tokens=max_length,  # More generous token limit
                top_p=0.9,
                stream=False  # Non-streaming for now, can add streaming later
            )
            
            # Get the summary from response
            if completion.choices and len(completion.choices) > 0:
                summary = completion.choices[0].message.content
                if summary and summary.strip():
                    summary = summary.strip()
                else:
                    print(f"Warning: Empty response from model {model}")
                    return f"Summary of {len(text)} characters [generated by {model}]"
            else:
                print(f"Warning: No choices in response from model {model}")
                return f"Unable to summarize text of {len(text)} characters"
            
            # Ensure summary doesn't exceed max_length
            if len(summary) > max_length:
                # Truncate at last sentence boundary
                sentences = summary.split('.')
                truncated = ""
                for sentence in sentences:
                    if len(truncated + sentence + '.') <= max_length:
                        truncated += sentence + '.'
                    else:
                        break
                summary = truncated.strip()
            
            logger.info(f"Generated summary: {len(summary)} chars from {len(text)} chars")
            return summary
            
        except Exception as e:
            logger.error(f"Groq summarization failed: {e}")
            raise
    
    async def summarize_text_streaming(
        self, 
        text: str, 
        max_length: int = 200,
        model: str = "llama-3.1-8b-instant",  # Current supported model
        callback: Optional[callable] = None
    ) -> str:
        """Generate summary using Groq LLM with streaming"""
        if not self.is_available():
            raise ValueError("Groq client not available - check API key")
        
        try:
            prompt = f"""Please summarize the following text in approximately {max_length} characters. 
Focus on the key points and main ideas. Make it concise but comprehensive.

Text to summarize:
{text}

Summary:"""

            # Create streaming completion
            completion = self.client.chat.completions.create(
                model=model,
                messages=[{
                    "role": "user",
                    "content": prompt
                }],
                temperature=0.3,
                max_tokens=min(max_length // 3, 500),
                top_p=0.9,
                stream=True
            )
            
            summary_parts = []
            
            for chunk in completion:
                if chunk.choices[0].delta.content:
                    content = chunk.choices[0].delta.content
                    summary_parts.append(content)
                    
                    # Call callback for real-time updates
                    if callback:
                        try:
                            await callback("llm_chunk", {
                                "chunk": content,
                                "partial_summary": "".join(summary_parts)
                            })
                        except Exception as e:
                            logger.warning(f"Callback error: {e}")
            
            summary = "".join(summary_parts).strip()
            
            # Ensure summary doesn't exceed max_length
            if len(summary) > max_length:
                sentences = summary.split('.')
                truncated = ""
                for sentence in sentences:
                    if len(truncated + sentence + '.') <= max_length:
                        truncated += sentence + '.'
                    else:
                        break
                summary = truncated.strip()
            
            logger.info(f"Generated streaming summary: {len(summary)} chars from {len(text)} chars")
            return summary
            
        except Exception as e:
            logger.error(f"Groq streaming summarization failed: {e}")
            raise
    
    async def refine_summary(
        self, 
        original_text: str,
        current_summary: str, 
        target_length: int = 200,
        model: str = "llama-3.1-8b-instant"  # Current supported model
    ) -> str:
        """Refine and improve an existing summary"""
        if not self.is_available():
            raise ValueError("Groq client not available - check API key")
        
        try:
            prompt = f"""Please refine and improve the following summary. Make it more concise while preserving the key information. Target length is approximately {target_length} characters.

Original text:
{original_text[:500]}...

Current summary:
{current_summary}

Please provide a refined summary that is:
1. More concise and focused
2. Approximately {target_length} characters
3. Preserves all key information

Refined summary:"""

            completion = self.client.chat.completions.create(
                model=model,
                messages=[{
                    "role": "user",
                    "content": prompt
                }],
                temperature=0.2,  # Very focused for refinement
                max_tokens=target_length // 3,
                top_p=0.8
            )
            
            refined_summary = completion.choices[0].message.content.strip()
            
            # Ensure refined summary doesn't exceed target_length
            if len(refined_summary) > target_length:
                sentences = refined_summary.split('.')
                truncated = ""
                for sentence in sentences:
                    if len(truncated + sentence + '.') <= target_length:
                        truncated += sentence + '.'
                    else:
                        break
                refined_summary = truncated.strip()
            
            logger.info(f"Refined summary: {len(refined_summary)} chars (target: {target_length})")
            return refined_summary
            
        except Exception as e:
            logger.error(f"Groq summary refinement failed: {e}")
            raise


# Global Groq client instance
groq_client = GroqClient()